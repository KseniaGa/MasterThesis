{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f3a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size, critic_dims, actor_dims, \n",
    "            n_actions, n_agents, batch_size):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.n_agents = n_agents\n",
    "        self.actor_dims = actor_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.reward_memory = np.zeros((self.mem_size, n_agents))\n",
    "        self.terminal_memory = np.zeros((self.mem_size, n_agents), dtype=bool)\n",
    "\n",
    "        self.init_actor_memory()\n",
    "\n",
    "    def init_actor_memory(self):\n",
    "        self.actor_state_memory = []\n",
    "        self.actor_new_state_memory = []\n",
    "        self.actor_action_memory = []\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self.actor_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_action_memory.append(\n",
    "                            np.zeros((self.mem_size, self.n_actions)))\n",
    "\n",
    "\n",
    "    def store_transition(self, raw_obs, state, action, reward, \n",
    "                               raw_obs_, state_, done):\n",
    "        # this introduces a bug: if we fill up the memory capacity and then\n",
    "        # zero out our actor memory, the critic will still have memories to access\n",
    "        # while the actor will have nothing but zeros to sample. Obviously\n",
    "        # not what we intend.\n",
    "        # In reality, there's no problem with just using the same index\n",
    "        # for both the actor and critic states. I'm not sure why I thought\n",
    "        # this was necessary in the first place. Sorry for the confusion!\n",
    "\n",
    "        #if self.mem_cntr % self.mem_size == 0 and self.mem_cntr > 0:\n",
    "        #    self.init_actor_memory()\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.actor_state_memory[agent_idx][index] = raw_obs[agent_idx]\n",
    "            self.actor_new_state_memory[agent_idx][index] = raw_obs_[agent_idx]\n",
    "            self.actor_action_memory[agent_idx][index] = action[agent_idx]\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        actor_states = []\n",
    "        actor_new_states = []\n",
    "        actions = []\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "\n",
    "        return actor_states, states, actions, rewards, \\\n",
    "               actor_new_states, states_, terminal\n",
    "\n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, \n",
    "                    n_agents, n_actions, name, chkpt_dir):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims+n_agents*n_actions, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.q = nn.Linear(fc2_dims, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, \n",
    "                 n_actions, name, chkpt_dir):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        pi = T.softmax(self.pi(x), dim=1)\n",
    "\n",
    "        return pi\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "#from networks import ActorNetwork, CriticNetwork\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, actor_dims, critic_dims, n_actions, n_agents, agent_idx, chkpt_dir,\n",
    "                    alpha=0.01, beta=0.01, fc1=64, \n",
    "                    fc2=64, gamma=0.95, tau=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions\n",
    "        self.agent_name = 'agent_%s' % agent_idx\n",
    "        self.actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions, \n",
    "                                  chkpt_dir=chkpt_dir,  name=self.agent_name+'_actor')\n",
    "        self.critic = CriticNetwork(beta, critic_dims, \n",
    "                            fc1, fc2, n_agents, n_actions, \n",
    "                            chkpt_dir=chkpt_dir, name=self.agent_name+'_critic')\n",
    "        self.target_actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions,\n",
    "                                        chkpt_dir=chkpt_dir, \n",
    "                                        name=self.agent_name+'_target_actor')\n",
    "        self.target_critic = CriticNetwork(beta, critic_dims, \n",
    "                                            fc1, fc2, n_agents, n_actions,\n",
    "                                            chkpt_dir=chkpt_dir,\n",
    "                                            name=self.agent_name+'_target_critic')\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        actions = self.actor.forward(state)\n",
    "        noise = T.rand(self.n_actions).to(self.actor.device)\n",
    "        action = actions + noise\n",
    "\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        actor_params = self.actor.named_parameters()\n",
    "\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2127f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "#from agent import Agent\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, actor_dims, critic_dims, n_agents, n_actions, \n",
    "                 scenario='simple',  alpha=0.01, beta=0.01, fc1=64, \n",
    "                 fc2=64, gamma=0.99, tau=0.01, chkpt_dir='tmp/maddpg/'):\n",
    "        self.agents = []\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        chkpt_dir += scenario \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.agents.append(Agent(actor_dims[agent_idx], critic_dims,  \n",
    "                            n_actions, n_agents, agent_idx, alpha=alpha, beta=beta,\n",
    "                            chkpt_dir=chkpt_dir))\n",
    "\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        for agent in self.agents:\n",
    "            agent.save_models()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        for agent in self.agents:\n",
    "            agent.load_models()\n",
    "\n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = []\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            action = agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def learn(self, memory):\n",
    "        if not memory.ready():\n",
    "            return\n",
    "\n",
    "        actor_states, states, actions, rewards, \\\n",
    "        actor_new_states, states_, dones = memory.sample_buffer()\n",
    "\n",
    "        device = self.agents[0].actor.device\n",
    "\n",
    "        states = T.tensor(states, dtype=T.float).to(device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(device)\n",
    "        rewards = T.tensor(rewards, dtype=T.float).to(device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(device)\n",
    "        dones = T.tensor(dones).to(device)\n",
    "\n",
    "        all_agents_new_actions = []\n",
    "        all_agents_new_mu_actions = []\n",
    "        old_agents_actions = []\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            new_states = T.tensor(actor_new_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "\n",
    "            new_pi = agent.target_actor.forward(new_states)\n",
    "\n",
    "            all_agents_new_actions.append(new_pi)\n",
    "            mu_states = T.tensor(actor_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "            pi = agent.actor.forward(mu_states)\n",
    "            all_agents_new_mu_actions.append(pi)\n",
    "            old_agents_actions.append(actions[agent_idx])\n",
    "\n",
    "        new_actions = T.cat([acts for acts in all_agents_new_actions], dim=1)\n",
    "        mu = T.cat([acts for acts in all_agents_new_mu_actions], dim=1)\n",
    "        old_actions = T.cat([acts for acts in old_agents_actions],dim=1)\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            critic_value_ = agent.target_critic.forward(states_, new_actions).flatten()\n",
    "            critic_value_[dones[:,0]] = 0.0\n",
    "            critic_value = agent.critic.forward(states, old_actions).flatten()\n",
    "\n",
    "            target = rewards[:,agent_idx] + agent.gamma*critic_value_\n",
    "            \n",
    "            critic_loss = F.mse_loss(target, critic_value)\n",
    "            agent.critic.optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            agent.critic.optimizer.step()\n",
    "\n",
    "            actor_loss = agent.critic.forward(states, mu).flatten()\n",
    "            actor_loss = -T.mean(actor_loss)\n",
    "            agent.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            agent.actor.optimizer.step()\n",
    "\n",
    "            agent.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0e14be",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 5]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11188/3229559907.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mmaddpg_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11188/1373490750.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL\\MADDPG\\multiagent-particle-envs\\maddpg\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL\\MADDPG\\multiagent-particle-envs\\maddpg\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 5]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from maddpg import MADDPG\n",
    "#from buffer import MultiAgentReplayBuffer\n",
    "from make_env import make_env\n",
    "\n",
    "def obs_list_to_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #scenario = 'simple'\n",
    "    scenario = 'simple_adversary'\n",
    "    env = make_env(scenario)\n",
    "    n_agents = env.n\n",
    "    actor_dims = []\n",
    "    for i in range(n_agents):\n",
    "        actor_dims.append(env.observation_space[i].shape[0])\n",
    "    critic_dims = sum(actor_dims)\n",
    "\n",
    "    # action space is a list of arrays, assume each agent has same action space\n",
    "    n_actions = env.action_space[0].n\n",
    "    maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n",
    "                           fc1=64, fc2=64,  \n",
    "                           alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                           chkpt_dir='tmp/maddpg/')\n",
    "\n",
    "    memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, \n",
    "                        n_actions, n_agents, batch_size=1024)\n",
    "\n",
    "    PRINT_INTERVAL = 500\n",
    "    N_GAMES = 50000\n",
    "    MAX_STEPS = 25\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    evaluate = False\n",
    "    best_score = 0\n",
    "\n",
    "    if evaluate:\n",
    "        maddpg_agents.load_checkpoint()\n",
    "\n",
    "    for i in range(N_GAMES):\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = [False]*n_agents\n",
    "        episode_step = 0\n",
    "        while not any(done):\n",
    "            if evaluate:\n",
    "                env.render()\n",
    "                #time.sleep(0.1) # to slow down the action for the video\n",
    "            actions = maddpg_agents.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(actions)\n",
    "\n",
    "            state = obs_list_to_state_vector(obs)\n",
    "            state_ = obs_list_to_state_vector(obs_)\n",
    "\n",
    "            if episode_step >= MAX_STEPS:\n",
    "                done = [True]*n_agents\n",
    "\n",
    "            memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "\n",
    "            if total_steps % 100 == 0 and not evaluate:\n",
    "                maddpg_agents.learn(memory)\n",
    "\n",
    "            obs = obs_\n",
    "\n",
    "            score += sum(reward)\n",
    "            total_steps += 1\n",
    "            episode_step += 1\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        if not evaluate:\n",
    "            if avg_score > best_score:\n",
    "                maddpg_agents.save_checkpoint()\n",
    "                best_score = avg_score\n",
    "        if i % PRINT_INTERVAL == 0 and i > 0:\n",
    "            print('episode', i, 'average score {:.1f}'.format(avg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed1357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(T.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maddpg",
   "language": "python",
   "name": "maddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
