{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a368752f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7844/1757405687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m \u001b[1;31m#for the activation functions?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #for the activation functions? \n",
    "import torch.optim as optim \n",
    "\n",
    "from make_env import make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e15f56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env('simple_adversary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8fc554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[Box(8,), Box(10,), Box(10,)]\n",
      "[Discrete(5), Discrete(5), Discrete(5)]\n",
      "5\n",
      "[array([ 0.37354364, -1.17757323,  0.91507418, -1.71695189,  1.06683316,\n",
      "       -1.11813814,  0.92040054, -0.91627065]), array([-0.15175897, -0.59881375, -0.69328952, -0.05943509, -0.15175897,\n",
      "       -0.59881375, -1.06683316,  1.11813814, -0.14643261,  0.2018675 ]), array([-0.00532636, -0.80068125, -0.5468569 , -0.26130258, -0.00532636,\n",
      "       -0.80068125, -0.92040054,  0.91627065,  0.14643261, -0.2018675 ])]\n"
     ]
    }
   ],
   "source": [
    "env = make_env('simple_adversary')\n",
    "print(env.n)\n",
    "#indexing is which agent we want? \n",
    "print(env.observation_space)\n",
    "#discrete? but we need cont \n",
    "print(env.action_space)\n",
    "#0 is do nothing and the rest is 'wasd'\n",
    "print(env.action_space[0].n)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "420c3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer:\n",
    "    \n",
    "    def __init__(self, max_size, critics_dims, actor_dims, n_actions, n_agents, batch_size):\n",
    "        \n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.n_agents = n_agents\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_dims = actor_dims\n",
    "        #self.critic_dims = critic_dims\n",
    "        \n",
    "        #critic/actor dims? \n",
    "        \n",
    "        #state memory of a critic? \n",
    "        self.state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        \n",
    "        self.reward_memory = np.zeros((self.mem_size, n_agents))\n",
    "        self.terminal_memory = np.zeros((self.mem_size, n_agents), dtype=bool)\n",
    "        \n",
    "        self.init_actor_memory()\n",
    "        \n",
    "    def init_actor_memory(self):\n",
    "        \n",
    "        self.actor_state_memory = []\n",
    "        self.actor_new_state_memory = []\n",
    "        \n",
    "        self.actor_action_memory = []\n",
    "        \n",
    "        for i in range(n_agents): \n",
    "            \n",
    "            self.actor_state_memory.append(np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            \n",
    "            self.actor_action_memory.append(np.zeros((self.mem_size, self.n_actions)))\n",
    "            \n",
    "    def store_transition(self, raw_obs, state, action, reward, raw_obs_, state_, done): \n",
    "        \n",
    "        if self.mem_cntr % self.mem_size == 0 and self.mem_cntr > 0 : \n",
    "            #when we reach the end of a memory we want to override the earliest memories \n",
    "            #buggy fixed in github? \n",
    "            self.init_actor_memory()\n",
    "            \n",
    "        #postiion of the first available memory\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        for agent_idx in range(self.n_agents) :\n",
    "            \n",
    "            self.actor_state_memory[agent_idx][index] = raw_obs[agent_idx]\n",
    "            self.actor_new_state_memory[agent_idx][index] = raw_obs_[agent_idx]\n",
    "            self.actor_action_memory[agent_idx][index] = action[agent_idx]\n",
    "            \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1 \n",
    "        \n",
    "    def sample_buffer(self): \n",
    "        \n",
    "        #what is the highest positon we filled in our memory? \n",
    "        #...because we don't want to sample a bunch of zeros\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        batch = random.choice(max_mem, self.batch_size, replace = False)\n",
    "        #batch = random.choice(max_mem, self.batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        actor_states = []\n",
    "        actor_new_states = []\n",
    "        actions = []\n",
    "        \n",
    "        for agent_idx in range(self.n_agents): \n",
    "            \n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "            \n",
    "        #difference between actor_states and states is that \n",
    "        #..actor_states are individual numpy arrays of length wither 8 or 10 or 10\n",
    "        #..and the states are the flatenned combination all three of those\n",
    "        #..that we use to pass into our critic \n",
    "        #.. and the actor states are what we use to pass to each individual actor for each agent\n",
    "        \n",
    "        return actor_states, states, actions, rewards, actor_new_states, states_, terminal\n",
    "            \n",
    "            \n",
    "    #function to determine weather we are allowed to sample a memory based on weather or not\n",
    "    #..we have filled up the batch size of our memories \n",
    "    \n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size: \n",
    "            return True \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82e1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module): \n",
    "    \n",
    "    #here we use the same learning rate for actor and critic - beta, but we can use separate ones\n",
    "    # fc - fully connected layers POG \n",
    "    \n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_agents, n_actions, name, chkpt_dir): \n",
    "        \n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "        \n",
    "        #our Critic is taking in full state observation vector of the whole system (28 dimensions - 8,10,10)\n",
    "        #..and then we have to feed in the full action vectors for each agent\n",
    "        #input dims is the full state vector size? input_dims = 28\n",
    "        #n_agents(3)*n_actions(5) = 15\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dims+n_agents*n_actions, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.q = nn.Linear(fc2_dims, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = beta)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        #cat? \n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "        \n",
    "    \n",
    "    def load_checkpoint(self): \n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b682958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir): \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "        \n",
    "        #input dims here are either 8 or 10, so not 28 \n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = alpha)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state): \n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = T.softmax(self.pi(x), dim = 1)\n",
    "        \n",
    "        return pi\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "        \n",
    "    def load_checkpoint(self): \n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bdff991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actor_dims, critic_dims, n_actions, agent_idx, chkpt_dir,\n",
    "                 alpha=0.01, beta=0.01, fc1=64, fc2=64, gamma = 0.95, tau = 0.01): \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions \n",
    "        self.agent_name = 'agent_%s' % agent_idx \n",
    "        \n",
    "        self.actor = ActorNetwork(alpha, actor_dims, fc1, fc2, \n",
    "                                  n_actions, name= self.agent_name + '_actor', chkpt_dir=chkpt_dir)\n",
    "        self.critic = CriticNetwork(beta, critic_dims, fc1, fc2, n_agents,\n",
    "                                    n_actions, name= self.agent_name + '_critic', chkpt_dir=chkpt_dir)\n",
    "        \n",
    "        self.target_actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions, \n",
    "                                         name= self.agent_name + '_target_actor', chkpt_dir=chkpt_dir)\n",
    "        self.target_critic = CriticNetwork(beta, critic_dims, fc1, fc2, n_agents, n_actions, \n",
    "                                           name=self.agent_name + '_target_critic', chkpt_dir=chkpt_dir)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        \n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        #actor\n",
    "        \n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        \n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        \n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + (1-tau)*target_actor_state_dict[name].clone()\n",
    "        \n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "        \n",
    "        #critic\n",
    "        \n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        \n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        \n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + (1-tau)*target_critic_state_dict[name].clone()\n",
    "        \n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "            \n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        #we are turning our observation into a pytorch tensor\n",
    "        state = T.tensor([observation], dtype = T.float).to(self.actor.device)\n",
    "        actions = self.actor.forward(state)\n",
    "        noise = T.rand(self.n_actions).to(self.actor.device)\n",
    "        action = actions + noise\n",
    "        \n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    \n",
    "    def save_models(self):\n",
    "        \n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_models(self):\n",
    "        \n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "151a6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MADDPG class - wrapper class that handles the fact that we have multiple agents \n",
    "\n",
    "class MADDPG:\n",
    "    \n",
    "    #creating a list of agents and calling the constructor for each agent\n",
    "    \n",
    "    def __init__(self, actor_dims, critic_dims, n_agents, n_actions, \n",
    "                scenario = 'simple', alpha=0.01, beta=0.01, fc1=64, fc2=64, \n",
    "                gamma = 0.99, tau = 0.01, chkpt_dir = 'tmp/maddpg/'):\n",
    "        \n",
    "        self.agents = []\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions \n",
    "        chkpt_dir += scenario \n",
    "        \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            \n",
    "            self.agents.append(Agent(actor_dims[agent_idx], critic_dims, n_actions, agent_idx, alpha=alpha, beta=beta, chkpt_dir=chkpt_dir))\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        \n",
    "        print('...saving checkpoint...')\n",
    "        for agent in self.agents: \n",
    "            agent.save_models()\n",
    "            \n",
    "    def load_checkpoint(self):  \n",
    "        print('...loading checkpoint...')\n",
    "        for agent in self.agents: \n",
    "            agent.load_models()       \n",
    "        \n",
    "        \n",
    "    #env expects a list of actions to the step function?\n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = []\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            action = agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "        \n",
    "    #memory will be a global memory replay buffer?\n",
    "    def learn(self, memory):\n",
    "        if not memory.ready():\n",
    "            return\n",
    "        \n",
    "        actor_states, states, actions, rewards, \\\n",
    "        actor_new_states, states_, dones = memory.sample_buffer()\n",
    "        \n",
    "        device = self.agents[0].actor.device\n",
    "        states = T.tensor(states, dtype=T.float).to(device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(device)\n",
    "        rewards = T.tensor(rawards, dtype=T.float).to(device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(device)\n",
    "        dones = T.tensor(dones).to(device)\n",
    "        \n",
    "        all_agents_new_actions = []\n",
    "        all_agents_new_mu_actions = []\n",
    "        old_agents_actions = []\n",
    "        \n",
    "        for agent_idx, agent in enumerate(self.agents): \n",
    "            \n",
    "            #we need all three different actions (actions according to the target network for the new states, \n",
    "            # ..., actions according to the actor network for the current states, \n",
    "            # the actions the agent actually took). \n",
    "            #we need them for the calculations of the loss function\n",
    "            \n",
    "            new_states = T.tensor(actor_new_states[agent_idx], dtype=T.float).to(device)\n",
    "            new_pi = agent.target_actor.forward(new_states)\n",
    "            all_agents_new_actions.append(new_pi)\n",
    "            \n",
    "            mu_states = T.tensor(actor_states[agent_idx], dtype=T.float).to(device)\n",
    "            pi = agent.actor.forward(mu_states)\n",
    "            all_agents_new_mu_actions.append(pi)\n",
    "            \n",
    "            old_agents_actions.append(actions[agent_idx])\n",
    "            \n",
    "            new_actions = T.cat([acts for acts in all_agents_new_actions], dim=1)\n",
    "            mu = T.cat([acts for acts in all_agents_new_mu_actions], dim=1)\n",
    "            old_actions = T.cat([acts for acts in old_agents_actions], dim=1)\n",
    "            \n",
    "            for agent_idx, agent in enumerate(self.agents):\n",
    "                \n",
    "                critic_value_ = agent.target_critic.forward(states_, new_actions).flatten()\n",
    "                critic_value_[dones[:, 0]] = 0.0\n",
    "                critic_value = agent.critic.forward(states, old_actions).flatten()\n",
    "                \n",
    "                #all rows for this particular agent \n",
    "                target = rewards[:, agent_idx] + agent.gamma*critic_value_\n",
    "                \n",
    "                critic_loss = F.mse_loss(target, critic_value)\n",
    "                \n",
    "                agent.critic.optimizer.zero_grad()\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                agent.critic.optimizer.step()\n",
    "                \n",
    "                #mu are the actions for the current states according to the regulart actor network -\n",
    "                # what action should we take given the new values of our actor network \n",
    "                # as opposed to the actions we actually took \n",
    "                \n",
    "                actor_loss = agent.critic.forward(states, mu).flatten()\n",
    "                actor_loss = -T.mean(actor_loss)\n",
    "                agent.actor.optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                agent.actor.optimizer.step()\n",
    "                \n",
    "                agent.update_network_parameters()\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f485f80a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "choice() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6780/2273277319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mmaddpg_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6780/2113219011.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mactor_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mactor_new_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6780/2615456757.py\u001b[0m in \u001b[0;36msample_buffer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#batch = random.choice(max_mem, self.batch_size, replace = False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: choice() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def obs_list_to_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #scenario = 'simple'\n",
    "    scenario = 'simple_adversary'\n",
    "    env = make_env(scenario)\n",
    "    n_agents = env.n\n",
    "    actor_dims = []\n",
    "    for i in range(n_agents):\n",
    "        actor_dims.append(env.observation_space[i].shape[0])\n",
    "    critic_dims = sum(actor_dims)\n",
    "\n",
    "    # action space is a list of arrays, assume each agent has same action space\n",
    "    n_actions = env.action_space[0].n\n",
    "    maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n",
    "                           fc1=64, fc2=64,  \n",
    "                           alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                           chkpt_dir='tmp/maddpg/')\n",
    "\n",
    "    memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, \n",
    "                        n_actions, n_agents, batch_size=1024)\n",
    "\n",
    "    PRINT_INTERVAL = 500\n",
    "    N_GAMES = 50000\n",
    "    MAX_STEPS = 25\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    evaluate = False\n",
    "    best_score = 0\n",
    "\n",
    "    if evaluate:\n",
    "        maddpg_agents.load_checkpoint()\n",
    "\n",
    "    for i in range(N_GAMES):\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = [False]*n_agents\n",
    "        episode_step = 0\n",
    "        while not any(done):\n",
    "            if evaluate:\n",
    "                env.render()\n",
    "                #time.sleep(0.1) # to slow down the action for the video\n",
    "            actions = maddpg_agents.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(actions)\n",
    "\n",
    "            state = obs_list_to_state_vector(obs)\n",
    "            state_ = obs_list_to_state_vector(obs_)\n",
    "\n",
    "            if episode_step >= MAX_STEPS:\n",
    "                done = [True]*n_agents\n",
    "\n",
    "            memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "\n",
    "            if total_steps % 100 == 0 and not evaluate:\n",
    "                maddpg_agents.learn(memory)\n",
    "\n",
    "            obs = obs_\n",
    "\n",
    "            score += sum(reward)\n",
    "            total_steps += 1\n",
    "            episode_step += 1\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        if not evaluate:\n",
    "            if avg_score > best_score:\n",
    "                maddpg_agents.save_checkpoint()\n",
    "                best_score = avg_score\n",
    "        if i % PRINT_INTERVAL == 0 and i > 0:\n",
    "            print('episode', i, 'average score {:.1f}'.format(avg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "073abd4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6780/1194692086.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mepisode_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#func will take each individual 8, 10 and 10 numpy array and turn it into a single numpy array of\n",
    "# 28 elements\n",
    "def obs_list_to_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scenario = 'simple'\n",
    "    env = make_env(scenario)\n",
    "    n_agents = env.n\n",
    "    actor_dims = []\n",
    "        \n",
    "    for i in range(n_agents): \n",
    "        actor_dims.append(env.observation_space[i].shape[0])\n",
    "    #28 for the critic \n",
    "    critic_dims = sum(actor_dims)\n",
    "        \n",
    "    #we assume that all agents have the same number of actions\n",
    "    n_actions = env.action_space[0].n\n",
    "    \n",
    "    maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n",
    "                           fc1=64, fc2=64, alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                          chkpt_dir = 'tmp/maddpg/')\n",
    "    \n",
    "    memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, \n",
    "                                   n_actions, n_agents, batch_size=1024)\n",
    "    \n",
    "    PRINT_INTERVAL = 500\n",
    "    N_GAMES = 30000\n",
    "    MAX_STEPS = 25\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    #waether or not we want to evaluate the performance of our agent\n",
    "    evaluate = False \n",
    "    best_score = 0\n",
    "    \n",
    "    if evaluate: \n",
    "        maddpg_agents.load_checkpoint()\n",
    "        \n",
    "    for i in range(N_GAMES): \n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = [False]*n_agents\n",
    "        episode_step = 0\n",
    "        while not any(done): \n",
    "            if evaluate:\n",
    "                env.render()\n",
    "            actions = maddpg_agents.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(actions)\n",
    "            \n",
    "            #??\n",
    "            state = obs_list_to_state_vector(obs)\n",
    "            state_ = obs_list_to_state_vector(obs_)\n",
    "            \n",
    "            if episode_step > MAX_STEPS: \n",
    "                done = True*n_agents\n",
    "                \n",
    "            memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "            \n",
    "            if total_steps % 100 == 0 and not evaluate: \n",
    "                \n",
    "                maddpg_agents.learn(memory)\n",
    "                \n",
    "            obs = obs_\n",
    "            \n",
    "            score += sum(reward)\n",
    "            total_steps += 1\n",
    "            episode_step += 1\n",
    "            \n",
    "        score_history.append(score)\n",
    "        #every previous 100 games\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        if not evaluate: \n",
    "            if avg_score > best_score: \n",
    "                maddpg_agents.save_checkpoint()\n",
    "                best_score = avg_score\n",
    "        if i % PRINT_INTERVAL == 0 and i > 0: \n",
    "            print('episode', i, 'average score {:.1f}'.format(avg_score))     \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7cb62c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "obs_list_to_state_vector() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6780/2587967091.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mobs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaddpg_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_list_to_state_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mstate_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaddpg_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_list_to_state_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: obs_list_to_state_vector() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    scenario = 'simple'\n",
    "    env = make_env(scenario)\n",
    "    n_agents = env.n\n",
    "    actor_dims = []\n",
    "        \n",
    "    for i in range(n_agents): \n",
    "        actor_dims.append(env.observation_space[i].shape[0])\n",
    "    #28 for the critic \n",
    "    critic_dims = sum(actor_dims)\n",
    "        \n",
    "    #we assume that all agents have the same number of actions\n",
    "    n_actions = env.action_space[0].n\n",
    "    \n",
    "    maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n",
    "                           fc1=64, fc2=64, alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                          chkpt_dir = 'tmp/maddpg/')\n",
    "    \n",
    "    memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, \n",
    "                                   n_actions, n_agents, batch_size=1024)\n",
    "    \n",
    "    PRINT_INTERVAL = 500\n",
    "    N_GAMES = 30000\n",
    "    MAX_STEPS = 25\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    #waether or not we want to evaluate the performance of our agent\n",
    "    evaluate = False \n",
    "    best_score = 0\n",
    "    \n",
    "    if evaluate: \n",
    "        maddpg_agents.load_checkpoint()\n",
    "        \n",
    "    for i in range(N_GAMES): \n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = [False]*n_agents\n",
    "        episode_step = 0\n",
    "        while not any(done): \n",
    "            if evaluate:\n",
    "                env.render()\n",
    "            actions = maddpg_agents.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(actions)\n",
    "            \n",
    "            #??\n",
    "            state = maddpg_agents.obs_list_to_state_vector(obs)\n",
    "            state_ = maddpg_agents.obs_list_to_state_vector(obs_)\n",
    "            \n",
    "            if episode_step > MAX_STEPS: \n",
    "                done = True*n_agents\n",
    "                \n",
    "            memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "            \n",
    "            if total_steps % 100 == 0 and not evaluate: \n",
    "                \n",
    "                maddpg_agents.learn(memory)\n",
    "                \n",
    "            obs = obs_\n",
    "            \n",
    "            score += sum(reward)\n",
    "            total_steps += 1\n",
    "            episode_step += 1\n",
    "            \n",
    "        score_history.append(score)\n",
    "        #every previous 100 games\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        if not evaluate: \n",
    "            if avg_score > best_score: \n",
    "                maddpg_agents.save_checkpoint()\n",
    "                best_score = avg_score\n",
    "        if i % PRINT_INTERVAL == 0 and i > 0: \n",
    "            print('episode', i, 'average score {:.1f}'.format(avg_score))\n",
    "            \n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maddpg",
   "language": "python",
   "name": "maddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
