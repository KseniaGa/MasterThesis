{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6228d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d938846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b168b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "from ray import rllib\n",
    "from make_env import make_env\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da73adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a35474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env import PettingZooEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "# import the pettingzoo environment\n",
    "from pettingzoo.butterfly import prison_v3\n",
    "# import rllib pettingzoo interface\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "# define how to make the environment. This way takes an optional environment config, num_floors\n",
    "env_creator = lambda config: prison_v3.env(num_floors=config.get(\"num_floors\", 4))\n",
    "# register that way to make the environment under an rllib name\n",
    "register_env('prison', lambda config: PettingZooEnv(env_creator(config)))\n",
    "# now you can use `prison` as an environment\n",
    "# you can pass arguments to the environment creator with the env_config option in the config\n",
    "config['env_config'] = {\"num_floors\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9196111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario_name:  simple_adversary2\n",
      "obs:  {0: array([ 1.10221274,  0.41361824,  1.60221274,  0.91361824,  1.63318088,\n",
      "        0.0902412 ,  1.03891653, -0.83966051,  1.40395872,  0.38698519,\n",
      "        1.51492634,  0.67618829,  1.31780425, -0.44456722,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ]), 1: array([-0.53096814,  0.32337704, -0.03096814,  0.82337704, -1.63318088,\n",
      "       -0.0902412 , -0.59426435, -0.92990171, -0.22922216,  0.29674399,\n",
      "       -0.11825454,  0.58594709, -0.31537663, -0.53480842,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ]), 2: array([ 0.06329621,  1.25327875,  0.56329621,  1.75327875, -1.03891653,\n",
      "        0.83966051,  0.59426435,  0.92990171,  0.36504219,  1.2266457 ,\n",
      "        0.47600981,  1.5158488 ,  0.27888772,  0.39509329,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ]), 3: array([-0.30174598,  0.02663305,  0.19825402,  0.52663305, -1.40395872,\n",
      "       -0.38698519,  0.22922216, -0.29674399, -0.36504219, -1.2266457 ,\n",
      "        0.11096762,  0.2892031 , -0.08615447, -0.83155241,  0.15      ,\n",
      "        0.65      ,  0.15      ]), 4: array([-0.4127136 , -0.26257005,  0.0872864 ,  0.23742995, -1.51492634,\n",
      "       -0.67618829,  0.11825454, -0.58594709, -0.47600981, -1.5158488 ,\n",
      "       -0.11096762, -0.2892031 , -0.19712209, -1.12075551,  0.15      ,\n",
      "        0.65      ,  0.15      ]), 5: array([-0.21559151,  0.85818546,  0.28440849,  1.35818546, -1.31780425,\n",
      "        0.44456722,  0.31537663,  0.53480842, -0.27888772, -0.39509329,\n",
      "        0.08615447,  0.83155241,  0.19712209,  1.12075551,  0.15      ,\n",
      "        0.65      ,  0.15      ])}\n",
      "{0: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf], (24,), float32), 1: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf], (24,), float32), 2: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf], (24,), float32), 3: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), 4: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), 5: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32)}\n",
      "{0: Discrete(5), 1: Discrete(5), 2: Discrete(5), 3: MultiDiscrete2, 4: MultiDiscrete2, 5: MultiDiscrete2}\n",
      "action_dict:  {0: array([0., 1., 0., 0., 0.]), 1: array([0., 0., 0., 0., 1.]), 2: array([0., 0., 1., 0., 0.])}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 82>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;66;03m#raise NotImplementedError\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dict: \u001b[39m\u001b[38;5;124m\"\u001b[39m, action_dict)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_dict\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mRLlibMultiAgentParticleEnv.step\u001b[1;34m(self, action_dict)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"Returns observations from ready agents.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mThe returns are dicts mapping from agent_id strings to values. The\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mnumber of agents in the env can vary over time.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m        Optional info values for each agent id.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(action_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 62\u001b[0m obs_list, rew_list, done_list, info_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m obs_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_dict(obs_list)\n\u001b[0;32m     65\u001b[0m rew_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_dict(rew_list)\n",
      "File \u001b[1;32m~\\Desktop\\RL\\MADDPG\\multiagent-particle-envs\\multiagent\\environment.py:88\u001b[0m, in \u001b[0;36mMultiAgentEnv.step\u001b[1;34m(self, action_n)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# set action for each agent\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_action(\u001b[43maction_n\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, agent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space[i])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# advance world state\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "from ray import rllib\n",
    "from make_env import make_env\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "class RLlibMultiAgentParticleEnv(rllib.MultiAgentEnv):\n",
    "    \"\"\"Wraps OpenAI Multi-Agent Particle env to be compatible with RLLib multi-agent.\"\"\"\n",
    "\n",
    "    def __init__(self, **mpe_args):\n",
    "        \"\"\"Create a new Multi-Agent Particle env compatible with RLlib.\n",
    "        Arguments:\n",
    "            mpe_args (dict): Arguments to pass to the underlying\n",
    "                make_env.make_env instance.\n",
    "        Examples:\n",
    "            >>> from rllib_env import RLlibMultiAgentParticleEnv\n",
    "            >>> env = RLlibMultiAgentParticleEnv(scenario_name=\"simple_reference\")\n",
    "            >>> print(env.reset())\n",
    "        \"\"\"\n",
    "\n",
    "        self._env = make_env(**mpe_args)\n",
    "        self.num_agents = self._env.n\n",
    "        self.agent_ids = list(range(self.num_agents))\n",
    "\n",
    "        self.observation_space_dict = self._make_dict(self._env.observation_space)\n",
    "        self.action_space_dict = self._make_dict(self._env.action_space)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the env and returns observations from ready agents.\n",
    "        Returns:\n",
    "            obs_dict: New observations for each ready agent.\n",
    "        \"\"\"\n",
    "\n",
    "        obs_dict = self._make_dict(self._env.reset())\n",
    "        return obs_dict\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \"\"\"Returns observations from ready agents.\n",
    "        The returns are dicts mapping from agent_id strings to values. The\n",
    "        number of agents in the env can vary over time.\n",
    "        Returns:\n",
    "            obs_dict:\n",
    "                New observations for each ready agent.\n",
    "            rew_dict:\n",
    "                Reward values for each ready agent.\n",
    "            done_dict:\n",
    "                Done values for each ready agent.\n",
    "                The special key \"__all__\" (required) is used to indicate env termination.\n",
    "            info_dict:\n",
    "                Optional info values for each agent id.\n",
    "        \"\"\"\n",
    "\n",
    "        actions = list(action_dict.values())\n",
    "        obs_list, rew_list, done_list, info_list = self._env.step(actions)\n",
    "\n",
    "        obs_dict = self._make_dict(obs_list)\n",
    "        rew_dict = self._make_dict(rew_list)\n",
    "        done_dict = self._make_dict(done_list)\n",
    "        done_dict[\"__all__\"] = all(done_list)\n",
    "        # FIXME: Currently, this is the best option to transfer agent-wise termination signal without touching RLlib code hugely.\n",
    "        # FIXME: Hopefully, this will be solved in the future.\n",
    "        info_dict = self._make_dict([{\"done\": done} for done in done_list])\n",
    "\n",
    "        return obs_dict, rew_dict, done_dict, info_dict\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        time.sleep(0.05)\n",
    "        self._env.render(mode=mode)\n",
    "\n",
    "    def _make_dict(self, values):\n",
    "        return dict(zip(self.agent_ids, values))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for scenario_name in [\"simple_adversary2\",\n",
    "                          \"simple\",\n",
    "                          \"simple_adversary\",\n",
    "                          \"simple_crypto\",\n",
    "                          \"simple_push\",\n",
    "                          \"simple_reference\",\n",
    "                          \"simple_speaker_listener\",\n",
    "                          \"simple_spread\",\n",
    "                          \"simple_tag\",\n",
    "                          \"simple_world_comm\"\n",
    "                         ]:\n",
    "        \n",
    "        print(\"scenario_name: \", scenario_name)\n",
    "        env = RLlibMultiAgentParticleEnv(scenario_name=scenario_name)\n",
    "        print(\"obs: \", env.reset())\n",
    "        print(env.observation_space_dict)\n",
    "        print(env.action_space_dict)\n",
    "\n",
    "        action_dict = {}\n",
    "        for i, ac_space in env.action_space_dict.items():\n",
    "            sample = ac_space.sample()\n",
    "            if isinstance(ac_space, Discrete):\n",
    "                action_dict[i] = np.zeros(ac_space.n)\n",
    "                action_dict[i][sample] = 1.0\n",
    "            elif isinstance(ac_space, Box):\n",
    "                action_dict[i] = sample\n",
    "            elif isinstance(ac_space, MultiDiscrete):\n",
    "                print(\"sample: \", sample)\n",
    "                print(\"ac_space: \", ac_space.nvec)\n",
    "                action_dict[i] = np.zeros(sum(ac_space.nvec))\n",
    "                start_ls = np.cumsum([0] + list(ac_space.nvec))[:-1]\n",
    "                for l in list(start_ls + sample):\n",
    "                    action_dict[i][l] = 1.0\n",
    "            #else:\n",
    "                #raise NotImplementedError\n",
    "\n",
    "        print(\"action_dict: \", action_dict)\n",
    "\n",
    "        for i in env.step(action_dict):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.contrib.maddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_trainable, register_env\n",
    "#from env import MultiAgentParticleEnv\n",
    "#RLlibMultiAgentParticleEnv\n",
    "import ray.rllib.contrib.maddpg.maddpg as maddpg\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "class CustomStdOut(object):\n",
    "    def _log_result(self, result):\n",
    "        if result[\"training_iteration\"] % 50 == 0:\n",
    "            try:\n",
    "                print(\"steps: {}, episodes: {}, mean episode reward: {}, agent episode reward: {}, time: {}\".format(\n",
    "                    result[\"timesteps_total\"],\n",
    "                    result[\"episodes_total\"],\n",
    "                    result[\"episode_reward_mean\"],\n",
    "                    result[\"policy_reward_mean\"],\n",
    "                    round(result[\"time_total_s\"] - self.cur_time, 3)\n",
    "                ))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            self.cur_time = result[\"time_total_s\"]\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\"MADDPG with OpenAI MPE\")\n",
    "\n",
    "    # Environment\n",
    "    parser.add_argument(\"--scenario\", type=str, default=\"simple\",\n",
    "                        choices=['simple', 'simple_speaker_listener',\n",
    "                                 'simple_crypto', 'simple_push',\n",
    "                                 'simple_tag', 'simple_spread', 'simple_adversary'],\n",
    "                        help=\"name of the scenario script\")\n",
    "    parser.add_argument(\"--max-episode-len\", type=int, default=25,\n",
    "                        help=\"maximum episode length\")\n",
    "    parser.add_argument(\"--num-episodes\", type=int, default=60000,\n",
    "                        help=\"number of episodes\")\n",
    "    parser.add_argument(\"--num-adversaries\", type=int, default=0,\n",
    "                        help=\"number of adversaries\")\n",
    "    parser.add_argument(\"--good-policy\", type=str, default=\"maddpg\",\n",
    "                        help=\"policy for good agents\")\n",
    "    parser.add_argument(\"--adv-policy\", type=str, default=\"maddpg\",\n",
    "                        help=\"policy of adversaries\")\n",
    "\n",
    "    # Core training parameters\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "                        help=\"learning rate for Adam optimizer\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.95,\n",
    "                        help=\"discount factor\")\n",
    "    # NOTE: 1 iteration = sample_batch_size * num_workers timesteps * num_envs_per_worker\n",
    "    parser.add_argument(\"--sample-batch-size\", type=int, default=25,\n",
    "                        help=\"number of data points sampled /update /worker\")\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=1024,\n",
    "                        help=\"number of data points /update\")\n",
    "    parser.add_argument(\"--n-step\", type=int, default=1,\n",
    "                        help=\"length of multistep value backup\")\n",
    "    parser.add_argument(\"--num-units\", type=int, default=64,\n",
    "                        help=\"number of units in the mlp\")\n",
    "\n",
    "    # Checkpoint\n",
    "    parser.add_argument(\"--checkpoint-freq\", type=int, default=7500,\n",
    "                        help=\"save model once every time this many iterations are completed\")\n",
    "    parser.add_argument(\"--local-dir\", type=str, default=\"./ray_results\",\n",
    "                        help=\"path to save checkpoints\")\n",
    "    parser.add_argument(\"--restore\", type=str, default=None,\n",
    "                        help=\"directory in which training state and model are loaded\")\n",
    "\n",
    "    # Parallelism\n",
    "    \n",
    "    #parser.add_argument(\"--num-workers\", type=int, default=1)\n",
    "    #parser.add_argument(\"--num-envs-per-worker\", type=int, default=4)\n",
    "    #parser.add_argument(\"--num-gpus\", type=int, default=0)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    ray.init(redis_max_memory=int(1e10), object_store_memory=int(3e9))\n",
    "    MADDPGAgent = maddpg.MADDPGTrainer.with_updates(\n",
    "        mixins=[CustomStdOut]\n",
    "    )\n",
    "    register_trainable(\"MADDPG\", MADDPGAgent)\n",
    "\n",
    "    def env_creater(mpe_args):\n",
    "        return MultiAgentParticleEnv(**mpe_args)\n",
    "\n",
    "    register_env(\"mpe\", env_creater)\n",
    "\n",
    "    env = env_creater({\n",
    "        \"scenario_name\": args.scenario,\n",
    "    })\n",
    "\n",
    "    def gen_policy(i):\n",
    "        use_local_critic = [\n",
    "            args.adv_policy == \"ddpg\" if i < args.num_adversaries else\n",
    "            args.good_policy == \"ddpg\" for i in range(env.num_agents)\n",
    "        ]\n",
    "        return (\n",
    "            None,\n",
    "            env.observation_space_dict[i],\n",
    "            env.action_space_dict[i],\n",
    "            {\n",
    "                \"agent_id\": i,\n",
    "                \"use_local_critic\": use_local_critic[i],\n",
    "                \"obs_space_dict\": env.observation_space_dict,\n",
    "                \"act_space_dict\": env.action_space_dict,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    policies = {\"policy_%d\" %i: gen_policy(i) for i in range(len(env.observation_space_dict))}\n",
    "    policy_ids = list(policies.keys())\n",
    "\n",
    "    run_experiments({\n",
    "        \"MADDPG_RLLib\": {\n",
    "            \"run\": \"MADDPG\",\n",
    "            \"env\": \"mpe\",\n",
    "            \"stop\": {\n",
    "                \"episodes_total\": args.num_episodes,\n",
    "            },\n",
    "            \"checkpoint_freq\": args.checkpoint_freq,\n",
    "            \"local_dir\": args.local_dir,\n",
    "            \"restore\": args.restore,\n",
    "            \"config\": {\n",
    "                # === Log ===\n",
    "                \"log_level\": \"ERROR\",\n",
    "\n",
    "                # === Environment ===\n",
    "                \"env_config\": {\n",
    "                    \"scenario_name\": args.scenario,\n",
    "                },\n",
    "                \"num_envs_per_worker\": args.num_envs_per_worker,\n",
    "                \"horizon\": args.max_episode_len,\n",
    "\n",
    "                # === Policy Config ===\n",
    "                # --- Model ---\n",
    "                \"good_policy\": args.good_policy,\n",
    "                \"adv_policy\": args.adv_policy,\n",
    "                \"actor_hiddens\": [args.num_units] * 2,\n",
    "                \"actor_hidden_activation\": \"relu\",\n",
    "                \"critic_hiddens\": [args.num_units] * 2,\n",
    "                \"critic_hidden_activation\": \"relu\",\n",
    "                \"n_step\": args.n_step,\n",
    "                \"gamma\": args.gamma,\n",
    "\n",
    "                # --- Exploration ---\n",
    "                \"tau\": 0.01,\n",
    "\n",
    "                # --- Replay buffer ---\n",
    "                \"buffer_size\": int(1e6),\n",
    "\n",
    "                # --- Optimization ---\n",
    "                \"actor_lr\": args.lr,\n",
    "                \"critic_lr\": args.lr,\n",
    "                \"learning_starts\": args.train_batch_size * args.max_episode_len,\n",
    "                \"sample_batch_size\": args.sample_batch_size,\n",
    "                \"train_batch_size\": args.train_batch_size,\n",
    "                \"batch_mode\": \"truncate_episodes\",\n",
    "\n",
    "                # --- Parallelism ---\n",
    "                \"num_workers\": args.num_workers,\n",
    "                \"num_gpus\": args.num_gpus,\n",
    "                \"num_gpus_per_worker\": 0,\n",
    "\n",
    "                # === Multi-agent setting ===\n",
    "                \"multiagent\": {\n",
    "                    \"policies\": policies,\n",
    "                    \"policy_mapping_fn\": ray.tune.function(\n",
    "                        lambda i: policy_ids[i]\n",
    "                    )\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f956c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contributed port of MADDPG from OpenAI baselines.\n",
    "The implementation has a couple assumptions:\n",
    "- The number of agents is fixed and known upfront.\n",
    "- Each agent is bound to a policy of the same name.\n",
    "- Discrete actions are sent as logits (pre-softmax).\n",
    "For a minimal example, see rllib/examples/two_step_game.py,\n",
    "and the README for how to run with the multi-agent particle envs.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Type\n",
    "\n",
    "from ray.rllib.agents.trainer import COMMON_CONFIG, with_common_config\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer\n",
    "from ray.rllib.contrib.maddpg.maddpg_policy import MADDPGTFPolicy\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch, MultiAgentBatch\n",
    "from ray.rllib.utils import merge_dicts\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# fmt: off\n",
    "# __sphinx_doc_begin__\n",
    "DEFAULT_CONFIG = with_common_config({\n",
    "    # === Framework to run the algorithm ===\n",
    "    \"framework\": \"tf\",\n",
    "\n",
    "    # === Settings for each individual policy ===\n",
    "    # ID of the agent controlled by this policy\n",
    "    \"agent_id\": None,\n",
    "    # Use a local critic for this policy.\n",
    "    \"use_local_critic\": False,\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # Evaluation interval\n",
    "    \"evaluation_interval\": None,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_duration\": 10,\n",
    "\n",
    "    # === Model ===\n",
    "    # Apply a state preprocessor with spec given by the \"model\" config option\n",
    "    # (like other RL algorithms). This is mostly useful if you have a weird\n",
    "    # observation shape, like an image. Disabled by default.\n",
    "    \"use_state_preprocessor\": False,\n",
    "    # Postprocess the policy network model output with these hidden layers. If\n",
    "    # use_state_preprocessor is False, then these will be the *only* hidden\n",
    "    # layers in the network.\n",
    "    \"actor_hiddens\": [64, 64],\n",
    "    # Hidden layers activation of the postprocessing stage of the policy\n",
    "    # network\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    # Postprocess the critic network model output with these hidden layers;\n",
    "    # again, if use_state_preprocessor is True, then the state will be\n",
    "    # preprocessed by the model specified with the \"model\" config option first.\n",
    "    \"critic_hiddens\": [64, 64],\n",
    "    # Hidden layers activation of the postprocessing state of the critic.\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    # N-step Q learning\n",
    "    \"n_step\": 1,\n",
    "    # Algorithm for good policies.\n",
    "    \"good_policy\": \"maddpg\",\n",
    "    # Algorithm for adversary policies.\n",
    "    \"adv_policy\": \"maddpg\",\n",
    "\n",
    "    # === Replay buffer ===\n",
    "    # Size of the replay buffer. Note that if async_updates is set, then\n",
    "    # each worker will have a replay buffer of this size.\n",
    "    \"buffer_size\": DEPRECATED_VALUE,\n",
    "    \"replay_buffer_config\": {\n",
    "        \"type\": \"MultiAgentReplayBuffer\",\n",
    "        \"capacity\": int(1e6),\n",
    "    },\n",
    "    # Observation compression. Note that compression makes simulation slow in\n",
    "    # MPE.\n",
    "    \"compress_observations\": False,\n",
    "    # If set, this will fix the ratio of replayed from a buffer and learned on\n",
    "    # timesteps to sampled from an environment and stored in the replay buffer\n",
    "    # timesteps. Otherwise, the replay will proceed at the native ratio\n",
    "    # determined by (train_batch_size / rollout_fragment_length).\n",
    "    \"training_intensity\": None,\n",
    "    # Force lockstep replay mode for MADDPG.\n",
    "    \"multiagent\": merge_dicts(COMMON_CONFIG[\"multiagent\"], {\n",
    "        \"replay_mode\": \"lockstep\",\n",
    "    }),\n",
    "\n",
    "    # === Optimization ===\n",
    "    # Learning rate for the critic (Q-function) optimizer.\n",
    "    \"critic_lr\": 1e-2,\n",
    "    # Learning rate for the actor (policy) optimizer.\n",
    "    \"actor_lr\": 1e-2,\n",
    "    # Update the target network every `target_network_update_freq` steps.\n",
    "    \"target_network_update_freq\": 0,\n",
    "    # Update the target by \\tau * policy + (1-\\tau) * target_policy\n",
    "    \"tau\": 0.01,\n",
    "    # Weights for feature regularization for the actor\n",
    "    \"actor_feature_reg\": 0.001,\n",
    "    # If not None, clip gradients during optimization at this value\n",
    "    \"grad_norm_clipping\": 0.5,\n",
    "    # How many steps of the model to sample before learning starts.\n",
    "    \"learning_starts\": 1024 * 25,\n",
    "    # Update the replay buffer with this many samples at once. Note that this\n",
    "    # setting applies per-worker if num_workers > 1.\n",
    "    \"rollout_fragment_length\": 100,\n",
    "    # Size of a batched sampled from replay buffer for training. Note that\n",
    "    # if async_updates is set, then each worker returns gradients for a\n",
    "    # batch of this size.\n",
    "    \"train_batch_size\": 1024,\n",
    "    # Number of env steps to optimize for before returning\n",
    "    \"timesteps_per_iteration\": 0,\n",
    "\n",
    "    # === Parallelism ===\n",
    "    # Number of workers for collecting samples with. This only makes sense\n",
    "    # to increase if your environment is particularly slow to sample, or if\n",
    "    # you're using the Async or Ape-X optimizers.\n",
    "    \"num_workers\": 1,\n",
    "    # Prevent iterations from going lower than this time span\n",
    "    \"min_time_s_per_reporting\": 0,\n",
    "})\n",
    "# __sphinx_doc_end__\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def before_learn_on_batch(multi_agent_batch, policies, train_batch_size):\n",
    "    samples = {}\n",
    "\n",
    "    # Modify keys.\n",
    "    for pid, p in policies.items():\n",
    "        i = p.config[\"agent_id\"]\n",
    "        keys = multi_agent_batch.policy_batches[pid].keys()\n",
    "        keys = [\"_\".join([k, str(i)]) for k in keys]\n",
    "        samples.update(dict(zip(keys, multi_agent_batch.policy_batches[pid].values())))\n",
    "\n",
    "    # Make ops and feed_dict to get \"new_obs\" from target action sampler.\n",
    "    new_obs_ph_n = [p.new_obs_ph for p in policies.values()]\n",
    "    new_obs_n = list()\n",
    "    for k, v in samples.items():\n",
    "        if \"new_obs\" in k:\n",
    "            new_obs_n.append(v)\n",
    "\n",
    "    for i, p in enumerate(policies.values()):\n",
    "        feed_dict = {new_obs_ph_n[i]: new_obs_n[i]}\n",
    "        new_act = p.get_session().run(p.target_act_sampler, feed_dict)\n",
    "        samples.update({\"new_actions_%d\" % i: new_act})\n",
    "\n",
    "    # Share samples among agents.\n",
    "    policy_batches = {pid: SampleBatch(samples) for pid in policies.keys()}\n",
    "    return MultiAgentBatch(policy_batches, train_batch_size)\n",
    "\n",
    "\n",
    "class MADDPGTrainer(DQNTrainer):\n",
    "    @classmethod\n",
    "    @override(DQNTrainer)\n",
    "    def get_default_config(cls) -> TrainerConfigDict:\n",
    "        return DEFAULT_CONFIG\n",
    "\n",
    "    @override(DQNTrainer)\n",
    "    def validate_config(self, config: TrainerConfigDict) -> None:\n",
    "        \"\"\"Adds the `before_learn_on_batch` hook to the config.\n",
    "        This hook is called explicitly prior to TrainOneStep() in the execution\n",
    "        setups for DQN and APEX.\n",
    "        \"\"\"\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        def f(batch, workers, config):\n",
    "            policies = dict(\n",
    "                workers.local_worker().foreach_policy_to_train(lambda p, i: (i, p))\n",
    "            )\n",
    "            return before_learn_on_batch(batch, policies, config[\"train_batch_size\"])\n",
    "\n",
    "        config[\"before_learn_on_batch\"] = f\n",
    "\n",
    "    @override(DQNTrainer)\n",
    "    def get_default_policy_class(self, config: TrainerConfigDict) -> Type[Policy]:\n",
    "        return MADDPGTFPolicy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
